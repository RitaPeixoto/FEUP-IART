**1.**
* uma raposa, uma galinha, uma cesta de milho
* um barco que so pode transportar um deles
* raposa nao pode ficar com a galinha, nem a galinha com o milho

**a)**

![statespace](./img/1a_2018_R.png)


**b)**

Uma função heuristica admissivel seria:

h = 2n, se o barco estiver na margem da esquerda

h = 2n - 1, se o barco estiver na margem da direita

Em que n é o numero de coisas que estao na margem inicial. Para transferir cada coisa é necessário trazer o barco e ir com a coisa. O custo será maior se for necessário trazer de volta um dos objetos.

h(RGMb||) = 6   h(RM||Gb) = 3  h(||RGMb) = 0

**c)**

![tree](./img/1c_2018_ER.png)


**2.**


**a)** C(S1) = max(10 + 7 +8 , 11 + 12) = 25

**b)** 

*  S2 = {A-M2, B-M1, C-M2, D-M2, E-M1}
    - C(S2) = max(7+8, 10+11+12) = 33

*  S3 = {A-M1, B-M3, C-M2, D-M2, E-M1}
    - C(S3) = max(10+8, 11+12, 7) = 23

*  S4 = {A-M1, B-M1, C-M3, D-M2, E-M1}
    - C(S4) = max(10+7+8, 12,11) = 25

*  S5 = {A-M1, B-M1, C-M2, D-M3, E-M1}
    - C(S5) = max(10+7+8, 11,12) = 25

*  S6 = {A-M1, B-M1, C-M2, D-M2, E-M2}
    - C(S6) = max(10+7, 11+12+8) = 31

*  S7 = {A-M1, B-M1, C-M2, D-M2, E-M3}
    - C(S7) = max(10+7, 11+12, 8) = 23

A variante "steepest accent" gera todos os sucessores e escolhe aquele que é melhor avaliado. Neste contexto, melhor avaliado é o que tem menor custo, por isso o sucessor escolhido poderiam ser tanto o 3 como o 7.

**c)**

p(S2) = e^(25-33/10)= 0.449
 S2 é rejeitado pois 0.55 > 0.449

S3 é aceite pois 23 < 25.

Pelo algoritmo de arrefecimento simulado o sucessor de S1 adotado seria o S3.

**3.** (machine learning)

**a)**

E(S) = -3/8 log2(3/8) -5/8 log2(5/8) = 0.954 

**b)**

E(res/hair) = 4/8 [ -2/4 log2(2/4) -2/4 log2(2/4) ] + 3/8 [ -3/3 log2(3/3) ] + 1/8 [ -1/1 log2(1/1) ]=
0.5
E(res/height) = 3/8 [ -2/3 log2(2/3) -1/3 log2(1/3) ] + 2/8 [ -2/2 log2(2/2) ] + 3/8 [ -2/3 log2(2/3) -
1/3 log2(1/3) ]= 0.689
E(res/weight) = 2/8 [ -1/2 log2(1/2) -1/2 log2(1/2) ] + 3/8 [ -2/3 log2(2/3) -1/3 log2(1/3) ] + 3/8 [ -
1/3 log2(1/3) -2/3 log2(2/3) ]= 0.939
E(res/lotion) = 5/8 [ -3/5 log2(3/5) -2/5 log2(2/5) ] + 3/8 [ -3/3 log2(3/3) ] = 0.607
O atributo escolhido é “hair”. 


**c)**

infosep(hair) = -4/8 log2(4/8) -3/8 log2(3/8) -1/8 log2(1/8) = 1.406
infosep(height) = -3/8 log2(3/8) -2/8 log2(2/8) -3/8 log2(3/8) = 1.561
infosep(weight) = -2/8 log2(2/8) -3/8 log2(3/8) -3/8 log2(3/8) = 1.561
infosep(lotion) = -5/8 log2(5/8) -3/8 log2(3/8) = 0.954
GainRatio(hair) = (0.954 – 0.5) / 1.406 = 0.321
GainRatio(height) = (0.954 – 0.689) / 1.561 = 0.1699
GainRatio(weight) = (0.954 – 0.939) / 1.561 = 0.0098
GainRatio(lotion) = (0.954 – 0.607) / 0.954 = 0.364
O atributo escolhido é “lotion”. 


**d)**

Aplicando o atributo “lotion”:

- se lotion=“no” então result=“sunburned”: 2 erros em 5 → (e+1)/(n+2) = 3/7 = 0.429

- se lotion=“yes” então result=“none”: 0 erros em 3 → (e+1)/(n+2) = 1/5 = 0.2 


**4.**

**a)** O algoritmo de pesquisa em profundidade com profundidade limitada, sendo este limite a profundidade máxima, um vez que assim permite obter uma solução a uma profundidade inferior ao limite dado, mesmo que esta nao seja otima. Tem tambem poucos requisitos de memoria o que tambem é importante dado o grande fator de ramificaçao do problema.

**b)** h' = h / 1.1 = 0.909 * h 

**c)**

**d)**

Total = 10+15+27+30 = 82

p(C1) = 10/82 = 0.122

p(C2) = 15/82 = 0.183

p(C3) = 27/82 = 0.329

p(C4) = 30/82 = 0.366

**e)** Nós 9, 1, 2, 7

**f)** Regras de causais, uma vez que o agente conhece todo o ambiente, daí sabendo quais as causas de cada perceçao que tem do mesmo.

**g)**

nao sai
